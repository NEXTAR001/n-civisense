# N-ATLaS API

**N-ATLaS is a GPUâ€‘accelerated, productionâ€‘ready FastAPI service for:

* ğŸ’¬ Conversational LLM inference (streaming & nonâ€‘streaming)
* ğŸ™ï¸ Speechâ€‘toâ€‘Text (Whisper)
* ğŸ§  Contextual memory & retrieval (Redis)
* ğŸš€ Optimized deployment on **DigitalOcean GPU Droplets** using Docker

This repository is designed for **realâ€‘world deployment**, not demos or notebooks.

---

## âœ¨ Features

* **FastAPI backend** with async endpoints
* **Token streaming (SSE)** for chat responses
* **Hugging Face Transformers** (GPUâ€‘accelerated)
* **Whisper speechâ€‘toâ€‘text** with FFmpeg
* **Redis** for conversation memory & caching
* **Docker + Docker Compose** (reproducible builds)
* **NGINXâ€‘friendly** (buffering disabled for streaming)
* **Scales vertically** on A10 / A100 GPUs

---

## ğŸ§± Architecture Overview

```
Client (Web / Mobile)
        â”‚
        â”‚  HTTP / SSE
        â–¼
     NGINX
        â”‚
        â–¼
 FastAPI (Nâ€‘ATLaS)
        â”‚
        â”œâ”€â”€ LLM Inference (GPU)
        â”œâ”€â”€ Whisper STT (GPU)
        â””â”€â”€ Redis (Context / Cache)
```

---

## ğŸ“ Project Structure

```
n-atlas-api/
â”œâ”€â”€ app.py                 # FastAPI entry point
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile             # GPUâ€‘ready container
â”œâ”€â”€ docker-compose.yml     # API + Redis
â”œâ”€â”€ .env.example           # Environment variable template
â”œâ”€â”€ README.md              # This file
â””â”€â”€ utils/                 # Helpers (LLM, audio, memory, etc.)
```

---

## ğŸ” Environment Variables

Create a `.env` file **on the server** (never commit it).

```env
# Model
MODEL_NAME=NCAIR1/N-ATLaS
HF_TOKEN=your_huggingface_token

# Whisper
WHISPER_MODEL=base

# Runtime
USE_REMOTE_INFERENCE=false
MAX_CONCURRENT_GENERATIONS=1

# Redis
REDIS_URL=redis://redis:6379/0

# Server
HOST=0.0.0.0
PORT=8000

# Logging
LOG_LEVEL=info
```

---

## ğŸ“¦ Requirements

### Local (for development)

* Python 3.10+
* FFmpeg
* Redis
* NVIDIA GPU (optional but recommended)

### Production (recommended)

* **DigitalOcean GPU Droplet**
* Ubuntu 22.04
* Docker + Docker Compose
* NVIDIA Container Toolkit

---

## ğŸ³ Docker Setup (Recommended)

### Dockerfile (GPUâ€‘enabled)

* CUDA 12
* cuDNN 8
* Optimized for inference workloads

### dockerâ€‘compose.yml

Includes:

* `api` â€“ Nâ€‘ATLaS FastAPI service
* `redis` â€“ context & caching layer

GPU access is enabled via:

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]
```

---

## ğŸš€ Deployment (DigitalOcean GPU Droplet)

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/anthon793/n-atlas-api.git
cd n-atlas-api
```

### 2ï¸âƒ£ Configure environment

```bash
cp .env.example .env
nano .env
```

### 3ï¸âƒ£ Build & start services

```bash
docker compose build
docker compose up -d
```

### 4ï¸âƒ£ Verify

```bash
docker logs -f n-atlas-api
```

---

## ğŸŒ NGINX Configuration (CRITICAL)

Streaming **will break** if buffering is enabled.

```nginx
location / {
    proxy_pass http://127.0.0.1:8000;
    proxy_http_version 1.1;
    proxy_set_header Connection "";
    proxy_buffering off;
    proxy_cache off;
    chunked_transfer_encoding on;
}
```

---

## ğŸ“¡ API Endpoints

### Health Check

```
GET /health
```

Response:

```json
{"status": "ok"}
```

---

### Chat (Nonâ€‘Streaming)

```
POST /chat
```

```json
{
  "message": "Explain transformers",
  "session_id": "user123"
}
```

---

### Chat (Streaming â€“ SSE)

```
POST /chat/stream
```

* Returns tokenâ€‘byâ€‘token output
* Requires NGINX buffering disabled

---

### Audio Transcription

```
POST /audio/transcribe
```

* Accepts audio file (wav, mp3, m4a)
* Uses Whisper on GPU

---

## ğŸ§  Memory & Context

* Conversation history stored in **Redis**
* Sessionâ€‘based context (`session_id`)
* Automatic truncation to fit model limits

---

## âš™ï¸ Performance Notes

* Limit concurrent generations on small GPUs
* A10: `MAX_CONCURRENT_GENERATIONS=1â€“2`
* A100: `MAX_CONCURRENT_GENERATIONS=4+`
* Use `bitsandbytes` for lower VRAM usage

---

## ğŸ”’ Security Best Practices

* Never expose Redis publicly
* Use HTTPS (TLS) via NGINX
* Rotate Hugging Face tokens regularly
* Restrict firewall to ports 22, 80, 443

---

## ğŸ§ª Testing

```bash
curl http://localhost:8000/health
```

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello"}'
```

---

## ğŸ›£ï¸ Roadmap

* [ ] Authentication & rate limiting
* [ ] Multiâ€‘tenant session isolation
* [ ] Vector database integration
* [ ] Prometheus metrics
* [ ] CI/CD (GitHub Actions)

---

## ğŸ“œ License

MIT License

---

## ğŸ¤ Contributing

Pull requests are welcome.
Please open an issue for major changes before submitting.

---

## ğŸ“« Support

For deployment, scaling, or optimization questions:

* Open an issue
* Or contact the maintainer directly

---

**Nâ€‘ATLaS is built for real workloads, real users, and real GPUs.** ğŸš€
